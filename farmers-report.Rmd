---
title: "Farmers report"
subtitle: for the week prior `r Sys.Date()`
output:
  html_document: default
params:
  elastic_host: dummy
  elastic_username: dummy
  elastic_password: dummy
  static: FALSE
---

This is a report for farm efficiency among HumGen teams for the last week.

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)

source('src/table_helpers.R')
source('src/elastic_helpers.R')
source('src/plot_helpers.R')
source("src/constants.R")

knitr::opts_chunk$set(echo = FALSE)
```

```{r connect, include=FALSE}
library(elastic)
elastic_con <- connect(
  host = params$elastic_host,
  path = "",
  user = params$elastic_username,
  pwd = params$elastic_password,
  port = 19200,
  transport_schema = "http"
)

table_view_opts <- ifelse(params$static, 't', 'ftp')
```

## Number of failed jobs
```{r pie chart, echo=FALSE, fig.height=3, fig.width=3, include=!params$static}
b <- build_agg_query("Job")

res <- Search(elastic_con, index = index, body = b, asdf = T)

df <- parse_elastic_single_agg(res) %>%
  mutate_for_piechart()

piechart(df, count_field = 'doc_count', key_field = 'key', legend_name = 'Job status')
```

### per team
```{r failed stats, echo=FALSE, fig.height=4, fig.width=9}
b <- build_terms_query(fields = c("ACCOUNTING_NAME", "Job"))

res <- Search(elastic_con, index = index, body = b, asdf = T)

parse_elastic_multi_agg(res, column_names = c('accounting_name', 'job_status')) %>%
  rename_group_column() -> df

make_per_team_job_plot(df, static = params$static)
```

```{r echo=FALSE, include=!params$static}
df %>%
  tidyr::pivot_wider(id_cols = 'accounting_name', names_from = 'job_status', values_from = 'doc_count', values_fill = 0) %>%
  mutate(fail_rate = Failed / (Failed + Success)) %>%
  arrange(desc(Failed)) -> dt
make_dt(dt, table_view_opts = table_view_opts)
```

## Wasted resources
Resource is considered to be wasted if it is not used or its job failed.
For memory we assume that memory consumption under the peak value is not wasted.
```{r elastic agg query, echo=FALSE}
custom_aggs <- list(
  "cpu_avail_sec" = build_elastic_sub_agg("AVAIL_CPU_TIME_SEC", "sum"),
  "cpu_wasted_sec" = build_elastic_sub_agg("WASTED_CPU_SECONDS", "sum"),
  "mem_avail_mb_sec" = build_elastic_sub_agg("MEM_REQUESTED_MB_SEC", "sum"),
  "mem_wasted_mb_sec" = build_elastic_sub_agg("WASTED_MB_SECONDS", "sum"),
  "wasted_cost" = wasted_cost_agg
)

b <- build_terms_query(
  fields = c("ACCOUNTING_NAME", "NUM_EXEC_PROCS", "Job"),
  aggs = custom_aggs
)

res <- Search(elastic_con, index = index, body = b, asdf = T)

df <- parse_elastic_multi_agg(res, column_names = c('accounting_name', 'procs', 'job_status')) %>%
  select(-doc_count)
```

```{r echo=FALSE, include=!params$static}
df %>%
  group_by(accounting_name) %>%
  generate_efficiency_stats() %>%
  select(accounting_name, cpu_wasted_hrs, cpu_wasted_frac, mem_wasted_gb_hrs, mem_wasted_frac, wasted_cost) %>%
  rename_group_column() -> dt

make_dt(dt, table_view_opts = table_view_opts)
```

If process allocates only 1 cpu and uses a fraction of it, we still consider it as wasting resources.
However it would be difficult to optimize that process.
Let's assume that successful processes requiring 1 cpu do not waste cpu.
```{r echo=FALSE, fig.width=10}
df %>%
  mutate(
    mem_wasted_cost = mem_wasted_mb_sec * ram_mb_second,
    cpu_wasted_sec = ifelse(job_status == 'Success' & procs == 1, 0, cpu_wasted_sec),
    wasted_cost = ifelse(job_status == 'Success' & procs == 1, mem_wasted_cost, wasted_cost)
  ) %>%
  group_by(accounting_name) %>%
  generate_efficiency_stats() %>%
  select(accounting_name, cpu_avail_hrs, cpu_wasted_hrs, cpu_wasted_frac, mem_avail_gb_hrs, mem_wasted_gb_hrs, mem_wasted_frac, wasted_cost) %>%
  rename_group_column() -> dt

ranks <- generate_ranks(dt) %>% select(accounting_name, awesomeness)

dt %>%
  left_join(ranks, by = 'accounting_name') -> dt

dt %>%
  tidyr::pivot_longer(cols = c('cpu_wasted_hrs', 'mem_wasted_gb_hrs')) %>%
  make_wastage_plot(renamer = column_rename)
```
<div style="page-break-before: always;" />

Awesome-ness is a 0-to-10 complex score representing a team performance. A team gets score 10 if it has the lowest fraction
of wasted resources for both CPU and RAM. The more heavy jobs team submits the harder it is to be efficient and vice versa.
For this reason teams get penalised if they consume very little resources.

```{asis awesomeness formula, echo=!params$static}
$$
awesomeness = 10 \times \frac{rank_{wasted\ CPU\ fraction} + rank_{wasted\ RAM\ fraction}}{ 2 \times numer\ of\ LSF\ groups}
$$

$$
rank_{metric} = \begin{cases}
 metric \leq \dfrac {median(metric)} {2} ,& \dfrac {rank(-metric)} {2} \\
 otherwise  ,& rank(-metric)
\end{cases}
$$
```


```{r echo=FALSE}
if(params$static) {
  dt <- select(dt, -mem_wasted_gb_hrs, -cpu_wasted_hrs)
}

make_dt(dt, all_rows = params$static, table_view_opts = table_view_opts)
```

```{r get leading team, echo=FALSE}
hgi_groups <- c('hgi', 'mercury-grp')
dt %>%
  arrange(desc(wasted_cost)) %>%
  pull(accounting_name) %>%
  setdiff(hgi_groups) %>%
  first() -> leading_team
```

Let's see per user statistics for the leading non-HGI team: `r leading_team`
```{r per user stat, echo=FALSE}
if(leading_team %in% team_map$team_name){
  leading_team <- filter(team_map, team_name == leading_team)[['team_code']]
}

b <- list(
  query = build_humgen_query(
    filters = build_humgen_filters(
      custom_filters = list(
        "match_phrase" = list(
          "ACCOUNTING_NAME" = leading_team
        )
      )
    )
  )
)

res <- Search(
  elastic_con,
  index = index,
  time_scroll="1m",
  source = c('USER_NAME', 'Job',
             'NUM_EXEC_PROCS', 'AVAIL_CPU_TIME_SEC', 'WASTED_CPU_SECONDS',
             'MEM_REQUESTED_MB', 'MEM_REQUESTED_MB_SEC', 'WASTED_MB_SECONDS'),
  body = b,
  asdf = T,
  size = 10000
)

df <- pull_everything(elastic_con, res)
df %>%
  rename(
    cpu_avail_sec = AVAIL_CPU_TIME_SEC,
    mem_avail_mb_sec = MEM_REQUESTED_MB_SEC,
    mem_wasted_mb_sec = WASTED_MB_SECONDS
  ) %>%
  group_by(USER_NAME) %>%
  mutate(
    cpu_wasted_sec = ifelse(Job == 'Success' & NUM_EXEC_PROCS == 1, 0, WASTED_CPU_SECONDS),
    cpu_wasted_cost = cpu_wasted_sec * cpu_second,
    mem_wasted_cost = mem_wasted_mb_sec * ram_mb_second,
    wasted_cost = pmax(cpu_wasted_cost, mem_wasted_cost)
  ) %>%
  generate_efficiency_stats(
    extra_stats = list(
      number_of_jobs = quote(n()),
      wasted_cost = quote(sum(wasted_cost)),
      fail_rate = quote(sum(Job == 'Failed') / number_of_jobs)
    )
  ) -> dt

if(params$static){
  dt <- select(dt, -cpu_wasted_hrs, -mem_wasted_gb_hrs)
}

make_dt(dt, table_view_opts = table_view_opts)
```
