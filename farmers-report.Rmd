---
title: "Farmers report"
subtitle: for the week prior `r Sys.Date()`
output:
  html_document: default
params:
  elastic_host: dummy
  elastic_username: dummy
  elastic_password: dummy
  static: FALSE
---

This is a report for farm5 efficiency among HumGen teams for the last week.

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)

source('src/table_helpers.R')
source('src/elastic_helpers.R')

knitr::opts_chunk$set(echo = FALSE)
```

```{r connect, include=FALSE}
library(elastic)
index <- "user-data-ssg-isg-lsf-analytics-*"
elastic_con <- connect(
  host = params$elastic_host,
  path = "",
  user = params$elastic_username,
  pwd = params$elastic_password,
  port = 19200,
  transport_schema = "http"
)

table_view_opts <- ifelse(params$static, 't', 'ftp')
```


## Number of failed jobs
```{r pie chart, echo=FALSE, fig.height=3, fig.width=3, include=!params$static}
b <- list(
  "aggs" = list(
    "stats" = list(
      "terms" = list(
        "field" = "Job",
        "order" = list(
          "_count" = "desc"
        )
      )
    )
  ),
  "size" = 0,
  "query" = humgen_query
)

res <- Search(
  elastic_con,
  index = index,
  body = b,
  asdf = T
)

df <- res$aggregations$stats$buckets %>%
  arrange(key) %>%
  mutate(csum = rev(cumsum(rev(doc_count))),
         pos = doc_count/2 + lead(csum, 1),
         pos = if_else(is.na(pos), doc_count/2, pos))

ggplot(df, aes(x = '', fill = key, y = doc_count)) +
  geom_bar(stat = 'identity', width=1, color="white") +
  coord_polar("y", start=0) +
  labs(fill = 'Job status') +
  ggrepel::geom_label_repel(aes(y = pos, label = doc_count),
                            size = 4.5,
                            nudge_x = 1,
                            show.legend = FALSE) +
  theme_void()
```

### per team
```{r failed stats, echo=FALSE, fig.height=4, fig.width=9}
b <- list(
  "aggs" = list(
    "stats" = list(
      "multi_terms" = list(
        "terms" = list(
          list("field" = "ACCOUNTING_NAME"),
          list("field" = "Job")
        ),
        "size" = 100
      )
    )
  ),
  "size" = 0,
  "query" = humgen_query
)

res <- Search(
  elastic_con,
  index = index,
  body = b,
  asdf = T
)

df <- res$aggregations$stats$buckets %>%
    select(-key_as_string) %>%
    tidyr::hoist(.col = key, accounting_name = 1L, job_status = 2L) %>%
    rename_group_column()

p <- ggplot(df, aes(x = accounting_name, fill=job_status, y=doc_count)) +
  geom_bar(stat='identity') +
  labs(y = 'Number of jobs', fill = 'Job status', x = "Accounting name") +
  theme_bw()

if(params$static){
  p + theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust=1),
            axis.title.x=element_blank())
} else{
  p + coord_flip()
}
```

```{r echo=FALSE, include=!params$static}
df %>%
  tidyr::pivot_wider(id_cols = 'accounting_name', names_from = 'job_status', values_from = 'doc_count', values_fill = 0) %>%
  mutate(fail_rate = Failed / (Failed + Success)) %>%
  arrange(desc(Failed)) -> dt
make_dt(dt)
```

## Wasted resources
Resource is considered to be wasted if it is not used or its job failed.
For memory we assume that memory consumption under the peak value is not wasted.
```{r elastic agg query, echo=FALSE}
b <- list(
  "aggs" = list(
    "stats" = list(
      "multi_terms" = list(
        "terms" = list(
          list("field" = "ACCOUNTING_NAME"),
          list("field" = "NUM_EXEC_PROCS"),
          list("field" = "Job")
        ),
        "size" = 1000
      ),
      "aggs" = list(
        "cpu_avail_sec" = list(
          "sum" = list(
            "field" = "AVAIL_CPU_TIME_SEC"
          )
        ),
        "cpu_wasted_sec" = list(
          "sum" = list(
            "field" = "WASTED_CPU_SECONDS"
          )
        ),
        "mem_avail_mb_sec" = list(
          "sum" = list(
            "field" = "MEM_REQUESTED_MB_SEC"
          )
        ),
        "mem_wasted_mb_sec" = list(
          "sum" = list(
            "field" = "WASTED_MB_SECONDS"
          )
        ),
        "wasted_cost" = wasted_cost_agg
      )
    )
  ),
  "size" = 0,
  "query" = humgen_query
)

res <- Search(
  elastic_con,
  index = index,
  body = b,
  asdf = T
)

df <- res$aggregations$stats$buckets %>%
  select(-key_as_string, -doc_count) %>%
  tidyr::hoist(.col = key, accounting_name = 1L, procs = 2L, job_status = 3L) %>%
  rename_all(~gsub('.value', '', .))
```

```{r echo=FALSE}
df %>%
  group_by(accounting_name) %>%
  summarise_at(c('cpu_avail_sec', 'cpu_wasted_sec', 'mem_avail_mb_sec', 'mem_wasted_mb_sec', 'wasted_cost'), sum) %>%
  mutate(
    mem_wasted_frac = mem_wasted_mb_sec / mem_avail_mb_sec,
    cpu_wasted_frac = cpu_wasted_sec / cpu_avail_sec,
    cpu_wasted_hrs = cpu_wasted_sec / 60 / 60,
    mem_wasted_gb_hrs = mem_wasted_mb_sec / 1024 / 60 / 60
  ) %>%
  select(accounting_name, cpu_wasted_hrs, cpu_wasted_frac, mem_wasted_gb_hrs, mem_wasted_frac, wasted_cost) %>%
  rename_group_column() -> dt

make_dt(dt)
```

<div style="page-break-before: always;" />

If process allocates only 1 cpu and uses a fraction of it, we still consider it as wasting resources.
However it would be difficult to optimize that process.
Let's assume that successful processes requiring 1 cpu do not waste cpu.
(numbers for money below do not account CPU adjustment)
```{r echo=FALSE, fig.width=10}
df %>%
  mutate(cpu_wasted_sec = ifelse(job_status == 'Success' & procs == 1, 0, cpu_wasted_sec)) %>%
  group_by(accounting_name) %>%
  summarise_at(c('cpu_avail_sec', 'cpu_wasted_sec', 'mem_avail_mb_sec', 'mem_wasted_mb_sec', 'wasted_cost'), sum) %>%
  mutate(
    cpu_avail_hrs = cpu_avail_sec / 60 / 60,
    cpu_wasted_frac = cpu_wasted_sec / cpu_avail_sec,
    cpu_wasted_hrs = cpu_wasted_sec / 60 / 60,
    mem_avail_gb_hrs = mem_avail_mb_sec / 1024 / 60 / 60,
    mem_wasted_frac = mem_wasted_mb_sec / mem_avail_mb_sec,
    mem_wasted_gb_hrs = mem_wasted_mb_sec / 1024 / 60 / 60
  ) %>%
  select(accounting_name, cpu_avail_hrs, cpu_wasted_hrs, cpu_wasted_frac, mem_avail_gb_hrs, mem_wasted_gb_hrs, mem_wasted_frac, wasted_cost) %>%
  rename_group_column() -> dt

dt %>%
  tidyr::pivot_longer(cols = c('cpu_wasted_hrs', 'mem_wasted_gb_hrs')) %>%
  ggplot(aes(x=accounting_name, y=value, fill=name)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    facet_grid(.~name, scales='free_x', labeller = as_labeller(setNames(names(column_rename), column_rename))) +
    theme_bw() +
    theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank())

make_dt(dt)
```

```{r get leading team, echo=FALSE}
hgi_groups <- c('hgi', 'mercury-grp')
dt %>%
  arrange(desc(wasted_cost)) %>%
  pull(accounting_name) %>%
  setdiff(hgi_groups) %>%
  first() -> leading_team
```

Let's see per user statistics for the leading non-HGI team: `r leading_team`
```{r per user stat, echo=FALSE}
if(leading_team %in% team_map$team_name){
  leading_team <- filter(team_map, team_name == leading_team)[['team_code']]
}

b <- list(
  "query" = list(
    "bool" = list(
      "filter" = c(
        humgen_filters,
        list(
          list(
            "match_phrase" = list(
              "ACCOUNTING_NAME" = leading_team
            )
          )
        )
      )
    )
  )
)

res <- Search(
  elastic_con,
  index = index,
  time_scroll="1m",
  source = c('USER_NAME', 'Job',
             'AVAIL_CPU_TIME_SEC', 'NUM_EXEC_PROCS', 'WASTED_CPU_SECONDS',
             'MEM_REQUESTED_MB', 'MEM_REQUESTED_MB_SEC', 'WASTED_MB_SECONDS'),
  body = b,
  asdf = T,
  size = 10000
)

df <- pull_everything(elastic_con, res)
df %>%
  group_by(USER_NAME) %>%
  mutate(
    cpu_wasted_sec = ifelse(Job == 'Success' & NUM_EXEC_PROCS == 1, 0, WASTED_CPU_SECONDS),
    cpu_wasted_cost = cpu_wasted_sec * cpu_hour / 60 / 60,
    mem_wasted_cost = WASTED_MB_SECONDS * gb_ram_hour / 1024 / 60 / 60,
    wasted_cost = pmax(cpu_wasted_cost, mem_wasted_cost)
  ) %>%
  summarise(
    number_of_jobs = n(),
    fail_rate = sum(Job == 'Failed') / number_of_jobs,
    cpu_avail_hrs = sum(AVAIL_CPU_TIME_SEC) / 60 / 60,
    cpu_wasted_hrs = sum(cpu_wasted_sec) / 60 / 60,
    cpu_wasted_frac = cpu_wasted_hrs / cpu_avail_hrs,
    mem_avail_gb_hrs = sum(MEM_REQUESTED_MB_SEC) / 1024 / 60 / 60,
    mem_wasted_gb_hrs = sum(WASTED_MB_SECONDS) / 1024 / 60 / 60,
    mem_wasted_frac = mem_wasted_gb_hrs / mem_avail_gb_hrs,
    wasted_cost = sum(wasted_cost)
  ) -> dt

if(params$static){
  dt <- select(dt, -cpu_wasted_hrs, -mem_wasted_gb_hrs)
}

make_dt(dt)
```